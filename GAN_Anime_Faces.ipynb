{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80r-zAdKRSop"
   },
   "source": [
    "# Anime Face Generation\n",
    "In the final project, you'll learn to define and train a GAN on a dataset of anime faces. The goal is to obtain a generator network to generate images of anime faces that look very cute and cartoon!\n",
    "\n",
    "The final project includes several tasks, from loading data and training a GAN. At the end of the notebook, you will be able to visualize the results of the trained generator to understand its performance; the samples you generate should look like fairly anime faces with a small amount of noise.\n",
    "\n",
    "**In this project, we encourage students to think about how to improve the performance of GAN and the stability of training. Students can write the experimental results and analysis into the report. This part will be used as a bonus item.**\n",
    "\n",
    "## Download the data\n",
    "You can use this link [Anime Face dataset](https://www.kaggle.com/lunarwhite/anime-face-dataset-ntumlds) to download dataset for training your adversarial networks.\n",
    "\n",
    "This is an dataset consisting of 36.7k high-quality anime faces. We suggest that you utilize a GPU for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T10:18:18.377373Z",
     "iopub.status.busy": "2022-04-14T10:18:18.377113Z",
     "iopub.status.idle": "2022-04-14T10:18:18.391112Z",
     "shell.execute_reply": "2022-04-14T10:18:18.389151Z",
     "shell.execute_reply.started": "2022-04-14T10:18:18.377343Z"
    },
    "id": "OyyYiRw6RSor"
   },
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import torch, torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch import nn, optim\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.utils import save_image\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "def show_images(images, nmax=64):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n",
    "# define dataset address\n",
    "root = '../input'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMm9ty45RSos"
   },
   "source": [
    "## Visualize the input image\n",
    "Note that these are color images with 3 color channels (RGB) each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T10:18:34.904191Z",
     "iopub.status.busy": "2022-04-14T10:18:34.903477Z",
     "iopub.status.idle": "2022-04-14T10:18:41.802905Z",
     "shell.execute_reply": "2022-04-14T10:18:41.802257Z",
     "shell.execute_reply.started": "2022-04-14T10:18:34.904151Z"
    },
    "id": "J9V2-OqjRSot",
    "outputId": "972bceee-e918-4d9d-b60c-3a2f28571c2c"
   },
   "outputs": [],
   "source": [
    "# Randomly choose images to visualize\n",
    "img_list = []\n",
    "for root, dirs, files in os.walk(root, topdown=False):\n",
    "    for name in files:\n",
    "        img = os.path.join(root, name)\n",
    "        img_list.append(img)\n",
    "print(len(img_list))\n",
    "\n",
    "r_idx = random.sample(range(0,36740),5)\n",
    "plt.figure(figsize=(16,8))\n",
    "for i in range(len(r_idx)):\n",
    "    img = Image.open(img_list[r_idx[i]])\n",
    "    plt.subplot(1,5,i+1)\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xQ67n1aRSo0"
   },
   "source": [
    "## Pre-process and Load the Data\n",
    "Based on the previous knowledge, students are required to complete the code for preprocessing and data loading. \n",
    "### For preprocessing \n",
    "we recommend students to use `transforms` in `torchvision`. Students are required to fill in at least two items: `Resize` and `CenterCrop`. Students can also try other **data augumentation methods** and display the results in the final report as a **bonus item**.\n",
    "### Load the Data\n",
    "In this part, you need define your own `Dataset` to load training images. For GAN, you don't need to load label data. Thus, defining the Dataset is easier than previous assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T10:18:41.804966Z",
     "iopub.status.busy": "2022-04-14T10:18:41.804511Z",
     "iopub.status.idle": "2022-04-14T10:18:42.059182Z",
     "shell.execute_reply": "2022-04-14T10:18:42.058418Z",
     "shell.execute_reply.started": "2022-04-14T10:18:41.804926Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing methods\n",
    "# The batch_size is defined by the memory of GPU or CPU.\n",
    "batch_size = 64\n",
    "stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    "transform = transforms.Compose([transforms.Resize(64),\n",
    "                                transforms.CenterCrop(64),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(*stats)])\n",
    "def denorm(img_tensors):\n",
    "    return img_tensors * stats[1][0] + stats[0][0]\n",
    "# Dataset and Dataloader.\n",
    "\n",
    "class AnimeData(Dataset):\n",
    "    \"\"\"\n",
    "    Wrap the data into a Dataset class, and then pass it to the DataLoader\n",
    "    :__init__: Initialization data\n",
    "    :__getitem__: support the indexing such that dataset[i] can be used to get ith sample\n",
    "    :__len__: return the size of the dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data = ImageFolder(root, self.transform)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index][0]\n",
    "\n",
    "# Trainloader \n",
    "trainset = AnimeData(root, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9uNW_npRSo1"
   },
   "source": [
    "## Check your device and move data to device\n",
    "In this part, students can check whether the computer's GPU is available and move the data to the GPU (or CPU). We strongly recommend that students use GPU to speed up the program. \n",
    "\n",
    "`torch.cuda.is_available` can help us to check whether GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T10:18:42.060704Z",
     "iopub.status.busy": "2022-04-14T10:18:42.060276Z",
     "iopub.status.idle": "2022-04-14T10:18:42.115546Z",
     "shell.execute_reply": "2022-04-14T10:18:42.114732Z",
     "shell.execute_reply.started": "2022-04-14T10:18:42.060665Z"
    },
    "id": "nKOn3ErZRSo2"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLnUw4L1RSo2"
   },
   "source": [
    "## Define a GAN\n",
    "A GAN consists of two adversarial networks, a discriminator and a generator.\n",
    "### Discriminator\n",
    "Your first task will be to define the discriminator. This is a convolutional classifier like you've built before, only without any maxpooling layers. \n",
    "#### Exercise: Complete the Discriminator class\n",
    "* The inputs to the discriminator are 3x64x64 tensor images\n",
    "* The output should be a single value that will indicate whether a given image is real or fake\n",
    "\n",
    "An example of our discriminant model is as follows, and students can also define it by themselves, including adjusting the model structure and activation function. We recommend that students use `nn.Sequential` to define the model, which is more simple and intuitive.\n",
    "\n",
    "\n",
    "**Students can build models based on examples, but we suggest you try different models (including model structure and activation function). This will be regared as a bonus.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T10:18:44.536267Z",
     "iopub.status.busy": "2022-04-14T10:18:44.535544Z",
     "iopub.status.idle": "2022-04-14T10:18:47.356539Z",
     "shell.execute_reply": "2022-04-14T10:18:47.355791Z",
     "shell.execute_reply.started": "2022-04-14T10:18:44.536231Z"
    },
    "id": "HI--BMZKRSo2"
   },
   "outputs": [],
   "source": [
    "# Discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,inchannels):\n",
    "        super(Discriminator,self).__init__()\n",
    "        \"\"\"\n",
    "        Initialize the Discriminator Module\n",
    "        :param inchannels: The depth of the first convolutional layer\n",
    "        \"\"\"\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param x: The input to the neural network     \n",
    "        :return: Discriminator logits; the output of the neural network\n",
    "        \"\"\"\n",
    "        x = self.seq(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "        \n",
    "D=Discriminator(3).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opGsxM4eRSo3"
   },
   "source": [
    "## Generator\n",
    "\n",
    "The generator should upsample an input and generate a *new* image of the same size as our training data `3x64x64`. This should be mostly transpose convolutional layers `nn.ConvTranspose2d` with normalization applied to the outputs.\n",
    "\n",
    "#### Exercise: Complete the Generator model\n",
    "* The inputs to the generator are vectors of some length `latent_size`\n",
    "* The output should be a image of shape `3x64x64`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T10:18:47.359538Z",
     "iopub.status.busy": "2022-04-14T10:18:47.359072Z",
     "iopub.status.idle": "2022-04-14T10:18:52.378258Z",
     "shell.execute_reply": "2022-04-14T10:18:52.377656Z",
     "shell.execute_reply.started": "2022-04-14T10:18:47.359496Z"
    },
    "id": "NiZFQi4tRSo3",
    "outputId": "aca501f5-aa0c-4e12-ca0a-550f5e938f16"
   },
   "outputs": [],
   "source": [
    "# Generator model\n",
    "latent_size = 128\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self,latent_size):\n",
    "        super(Generator,self).__init__()\n",
    "        \"\"\"\n",
    "        Initialize the Generator Module\n",
    "        :param latent_size: The length of the input latent vector\n",
    "        \"\"\"\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=latent_size, out_channels=512, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=3, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param x: The input to the neural network     \n",
    "        :return: A 3x64x64 Tensor image as output\n",
    "        \"\"\"\n",
    "        x = self.seq(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "G=Generator(latent_size).to(device) \n",
    "\n",
    "# random latent tensors\n",
    "noise = torch.randn(batch_size, latent_size, 1, 1).to(device) \n",
    "\n",
    "# Using generator model to generate fake image \n",
    "fake_images = G(noise).to(device) \n",
    "print(fake_images.shape)\n",
    "\n",
    "# Visualizing the fake images by function show_images\n",
    "show_images(fake_images.to(torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e1HbC_MRSo4"
   },
   "source": [
    "---\n",
    "## Discriminator and Generator Losses\n",
    "\n",
    "Now we need to calculate the losses for both types of adversarial networks.\n",
    "\n",
    "### Discriminator Losses\n",
    "\n",
    "> * For the discriminator, the total loss is the sum of the losses for real and fake images, `d_loss = d_real_loss + d_fake_loss`. \n",
    "* Remember that we want the discriminator to output 1 for real images and 0 for fake images, so we need to set up the losses to reflect that.\n",
    "\n",
    "\n",
    "### Generator Loss\n",
    "\n",
    "The generator loss will look similar only with flipped labels. The generator's goal is to get the discriminator to *think* its generated images are *real*.\n",
    "\n",
    "#### Exercise: Complete real and fake loss functions\n",
    "\n",
    "**You may choose to use either cross entropy or a least squares error loss to complete the following `real_loss` and `fake_loss` functions. We also encourage students to use the loss function in other related papers as a bonus item. If you use it, please include a citation in the report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T10:18:52.379888Z",
     "iopub.status.busy": "2022-04-14T10:18:52.379496Z",
     "iopub.status.idle": "2022-04-14T10:18:52.385951Z",
     "shell.execute_reply": "2022-04-14T10:18:52.385267Z",
     "shell.execute_reply.started": "2022-04-14T10:18:52.379851Z"
    },
    "id": "mnPww9GjRSo4"
   },
   "outputs": [],
   "source": [
    "# Loss function for training GAN\n",
    "\n",
    "lossf = torch.nn.BCELoss()\n",
    "def Real_loss(preds,targets):\n",
    "    '''\n",
    "       Calculates how close discriminator outputs are to being real.\n",
    "       param, D_out: discriminator logits\n",
    "       return: real loss\n",
    "    '''\n",
    "    loss = lossf(preds,targets)\n",
    "    return loss\n",
    "def Fake_loss(preds,targets):\n",
    "    '''\n",
    "       Calculates how close discriminator outputs are to being fake.\n",
    "       param, D_out: discriminator logits\n",
    "       return: fake loss\n",
    "    '''\n",
    "    loss = lossf(preds,targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjdjbYP3RSo4"
   },
   "source": [
    "## Optimizers\n",
    "\n",
    "#### Exercise: Define optimizers for your Discriminator (D) and Generator (G)\n",
    "\n",
    "Define optimizers for your models with appropriate hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T10:18:52.388141Z",
     "iopub.status.busy": "2022-04-14T10:18:52.387767Z",
     "iopub.status.idle": "2022-04-14T10:18:52.403466Z",
     "shell.execute_reply": "2022-04-14T10:18:52.402506Z",
     "shell.execute_reply.started": "2022-04-14T10:18:52.388105Z"
    },
    "id": "_Z3nNpNqRSo4"
   },
   "outputs": [],
   "source": [
    "# Optimizers for the discriminator D and generator G\n",
    "\n",
    "lr = 0.0002\n",
    "opt_d = torch.optim.Adam(D.parameters(), lr=lr/2, betas=(0.5, 0.999),weight_decay = 0.0005)\n",
    "opt_g = torch.optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999),weight_decay = 0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twNcCwzLRSo5"
   },
   "source": [
    "### Save the generated images\n",
    "This code can help you save images generated from Generator G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T10:18:55.396342Z",
     "iopub.status.busy": "2022-04-14T10:18:55.395876Z",
     "iopub.status.idle": "2022-04-14T10:18:55.410859Z",
     "shell.execute_reply": "2022-04-14T10:18:55.409942Z",
     "shell.execute_reply.started": "2022-04-14T10:18:55.396301Z"
    },
    "id": "6vN0GKAyRSo5"
   },
   "outputs": [],
   "source": [
    "# Define save path.\n",
    "sample_dir = 'generated'\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "def save_samples(index, latent_tensors, generator, show=True):\n",
    "    fake_images = generator(latent_tensors)\n",
    "    fake_fname = 'generated-images-{0:0=4d}.png'.format(index)\n",
    "    save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=8)\n",
    "    print('Saving', fake_fname)\n",
    "    if show:\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.imshow(make_grid(fake_images.cpu().detach(), nrow=8).permute(1, 2, 0))\n",
    "        plt.show()\n",
    "        \n",
    "def save_samples2(index, latent_tensors, generator, show=True):\n",
    "    fake_images = generator(latent_tensors)\n",
    "    fake_fname = 'generated-images-{0:0=4d}.png'.format(index)\n",
    "    save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=8)\n",
    "    print('Saving', fake_fname)\n",
    "    if show:\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        imm=make_grid(fake_images.cpu().detach(), nrow=8).permute(1, 2, 0)\n",
    "        ax.imshow(imm)\n",
    "        plt.show()\n",
    "        plt.savefig('data.png')\n",
    "    return fig, imm\n",
    "fixed_latent = torch.randn(64, latent_size, 1, 1, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2-JBY08RSo5"
   },
   "source": [
    "## Training GAN to generate anime faces\n",
    "Training will involve alternating between training the discriminator and the generator. You'll use your functions `real_loss` and `fake_loss` to help you calculate the discriminator losses.\n",
    "\n",
    "* You should train the discriminator by alternating on real and fake images\n",
    "* Then the generator, which tries to trick the discriminator and should have an opposing loss function\n",
    "\n",
    "\n",
    "#### Saving Samples\n",
    "\n",
    "You've been given some code to save some generated \"fake\" samples.\n",
    "\n",
    "#### Exercise: Complete the training function\n",
    "\n",
    "Keep in mind that, if you've moved your models to GPU, you'll also have to move any model inputs to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-04-14T10:18:58.812756Z",
     "iopub.status.busy": "2022-04-14T10:18:58.812483Z",
     "iopub.status.idle": "2022-04-14T11:14:20.097023Z",
     "shell.execute_reply": "2022-04-14T11:14:20.096147Z",
     "shell.execute_reply.started": "2022-04-14T10:18:58.812726Z"
    },
    "id": "ikY52qQhRSo5",
    "outputId": "9332f1d3-8f1e-425b-e57b-82504b3c3ae2"
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "losses_g = []\n",
    "losses_d = []\n",
    "real_scores = []\n",
    "fake_scores = []\n",
    "\n",
    "def train(D, G, d_optimizer, g_optimizer, epochs=1):\n",
    "    iter_count = 0\n",
    "    start_idx = 1\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss_dr = []\n",
    "        epoch_loss_df = []\n",
    "        epoch_loss_g = []\n",
    "        real_h = []\n",
    "        fake_h = []\n",
    "        for real_images in tqdm(trainloader):\n",
    "            real_images=real_images.to(device)\n",
    "            \n",
    "            # Pass real images through discriminator\n",
    "            d_optimizer.zero_grad()\n",
    "            D_out_real = D(real_images)\n",
    "            real_labels = (0.9 - 1) * torch.rand_like(D_out_real).to(device) + 1\n",
    "            real_loss = Real_loss(D_out_real,real_labels)\n",
    "            \n",
    "            # Generate fake images\n",
    "            noise = torch.randn(batch_size, latent_size, 1, 1).to(device)\n",
    "            G_out = G(noise)\n",
    "            \n",
    "            # Pass fake images through discriminator\n",
    "            D_out_fake = D(G_out)\n",
    "            fake_labels = (0 - 0.1) * torch.rand_like(D_out_fake).to(device) + 0.1\n",
    "            fake_loss = Fake_loss(D_out_fake, fake_labels)\n",
    "            \n",
    "            # Update discriminator weights\n",
    "            loss_d = real_loss + fake_loss\n",
    "            loss_d.backward(retain_graph=True)\n",
    "            d_optimizer.step()\n",
    "            \n",
    "            ## 2. Train the generator with an adversarial loss\n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            # Try to fool the discriminator\n",
    "            D_out_fake2 = D(G_out)\n",
    "            \n",
    "            # The label is set to 1(real-like) to fool the discriminator\n",
    "            real_labels2 = torch.full(D_out_fake2.shape, 1.0).to(device)\n",
    "            loss_g = Real_loss(D_out_fake2, real_labels2)\n",
    "            \n",
    "            # Update generator weights\n",
    "            loss_g.backward()\n",
    "            g_optimizer.step()\n",
    "                \n",
    "            # Handle evaluation parameters\n",
    "            epoch_loss_g.append(loss_g.item())\n",
    "            epoch_loss_dr.append(real_loss.item())\n",
    "            epoch_loss_df.append(fake_loss.item())\n",
    "            epoch_loss_d = epoch_loss_dr + epoch_loss_df\n",
    "            real_h.append(np.sum(list(D_out_real.cpu().detach()))/len(D_out_real))\n",
    "            fake_h.append(np.sum(list(D_out_fake.cpu().detach()))/len(D_out_fake))\n",
    "            \n",
    "        # Losses of the latest batch\n",
    "        losses_g.append(np.mean(epoch_loss_d))\n",
    "        losses_d.append(np.mean(epoch_loss_g))\n",
    "\n",
    "        # Take mean score from the epoch\n",
    "        real_score = np.mean(real_h)\n",
    "        fake_score = np.mean(fake_h)\n",
    "        real_scores.append(real_score)\n",
    "        fake_scores.append(fake_score)\n",
    "        \n",
    "        # Log losses & scores (last batch)\n",
    "        print(\"Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(\n",
    "        epoch+1, epochs, loss_g, loss_d, real_score, fake_score))\n",
    "        \n",
    "        # Plot scores and losses for better analyzing \n",
    "        plt.figure()\n",
    "        plt.subplot(2,1,1)\n",
    "        plt.plot(epoch_loss_dr, label='d-real')\n",
    "        plt.plot(epoch_loss_df, label='d-fake')\n",
    "        plt.plot(epoch_loss_g, label='gen')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(2,1,2)\n",
    "        plt.plot(real_h, label='real-score')\n",
    "        plt.plot(fake_h, label='fake-score')\n",
    "        plt.legend()\n",
    "\n",
    "\n",
    "        # Save generated images\n",
    "        save_samples(epoch+start_idx, fixed_latent,G, show=True)\n",
    "        \n",
    "            \n",
    "        state_dis = {'dis_model': D.state_dict(), 'epoch': epoch}\n",
    "        state_gen = {'gen_model': G.state_dict(), 'epoch': epoch}\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint') \n",
    "        torch.save(state_dis, 'checkpoint/'+'D__'+str(epoch+1)) #each epoch\n",
    "        torch.save(state_gen, 'checkpoint/'+'G__'+str(epoch+1)) #each epoch\n",
    "        \n",
    "#Train the GAN\n",
    "train(D,G,opt_d,opt_g,epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T11:19:26.769967Z",
     "iopub.status.busy": "2022-04-14T11:19:26.769703Z",
     "iopub.status.idle": "2022-04-14T11:19:27.008620Z",
     "shell.execute_reply": "2022-04-14T11:19:27.007954Z",
     "shell.execute_reply.started": "2022-04-14T11:19:26.769937Z"
    },
    "id": "O9OBcVOHRSo6"
   },
   "outputs": [],
   "source": [
    "# Visualize loss curve of D and G\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(losses_g, label='Discriminator', alpha=0.5)\n",
    "plt.plot(losses_d, label='Generator', alpha=0.5)\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-14T11:19:39.319394Z",
     "iopub.status.busy": "2022-04-14T11:19:39.318854Z",
     "iopub.status.idle": "2022-04-14T11:19:39.545814Z",
     "shell.execute_reply": "2022-04-14T11:19:39.545159Z",
     "shell.execute_reply.started": "2022-04-14T11:19:39.319356Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.plot(real_scores, label='Real score', alpha=0.5)\n",
    "plt.plot(fake_scores, label='Fake score', alpha=0.5)\n",
    "plt.title(\"Training Scores\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-23T20:34:57.775305Z",
     "iopub.status.busy": "2021-12-23T20:34:57.774912Z",
     "iopub.status.idle": "2021-12-23T20:34:58.091557Z",
     "shell.execute_reply": "2021-12-23T20:34:58.090835Z",
     "shell.execute_reply.started": "2021-12-23T20:34:57.775268Z"
    }
   },
   "outputs": [],
   "source": [
    "asd = G(noise).cpu()\n",
    "f,imm = save_samples2(1, fixed_latent,G, show=True)\n",
    "#plt.savefig(data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4zHx1phRSo6"
   },
   "source": [
    "### Question1: What do you notice about your generated samples and how might you improve this model?\n",
    "When you answer this question, consider the following factors:\n",
    "* Model size; larger models have the opportunity to learn more features in a data feature space\n",
    "* Optimization strategy; optimizers and number of epochs affect your final result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HH6yS7PURSo6"
   },
   "source": [
    "Samples created got slowly better, but in the latter epochs there wasn't any significant improvement to be seen. Deeper model would benefit from greater number of epochs as it would be able to optimize more parameters. Optimizer selected and its parameters would be crucial to prevent vanishing gradients and other unwanted failures by keeping the training stable. Adam optimizer seems to be the most used with DCGANs but one might try to apply SGD optimizer to the discriminator. \n",
    "\n",
    "To improve this model I would introduce dropout to the discriminator layers. I would also try out multiple different methods too, for example adding noise to the discriminator layers and continuing searching for the optimal Adam parameters. Maybe even smaller learning rates would produce better images as the model currently reaches its best results way before 40 epoch deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9xOmKZURSo7"
   },
   "source": [
    "### Question2: How does the training loss of the generator and the discriminator change during your training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpXacPfzRSo7"
   },
   "source": [
    "Training loss of discriminator stays low. A bit too low. Generator loss isquite stable also, but they are quite far from each other to provide optimal training and performance of the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFPrlrPyRSo7"
   },
   "source": [
    "### Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as \"final_project_StudentNumber_StudentName.ipynb\". Include the generated images in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgKzkAa1RSo7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
